{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1.Build spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2.Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = spark.read.option(\"inferSchema\", \"true\").csv(\"Car_Classification_Train.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+---------+------------+-----------+-------+----+\n",
      "|    Brand|Model Year|Mileage|Fuel Type|Transmission|Owner Count|  Price|Sold|\n",
      "+---------+----------+-------+---------+------------+-----------+-------+----+\n",
      "|    Honda|      2019|  99704|   Petrol|   Automatic|          4|37884.0| Yes|\n",
      "|   Toyota|      2015|  11801|   Hybrid|   Automatic|          3|29737.0| Yes|\n",
      "|   Toyota|      2015|  74042|   Petrol|   Automatic|          3|34273.0| Yes|\n",
      "|   Toyota|      2007|  25056|   Petrol|   Automatic|          2|37016.0|  No|\n",
      "|Chevrolet|      2019|  12543|   Hybrid|      Manual|          1|22256.0| Yes|\n",
      "+---------+----------+-------+---------+------------+-----------+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.read.option(\"inferSchema\", \"true\").csv(\"Car_Classification_Test.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+---------+------------+-----------+-------+----+\n",
      "|    Brand|Model Year|Mileage|Fuel Type|Transmission|Owner Count|  Price|Sold|\n",
      "+---------+----------+-------+---------+------------+-----------+-------+----+\n",
      "|      BMW|      2018| 156836|     null|   Automatic|          3|20368.0| Yes|\n",
      "|     null|      2018| 105235|   Diesel|   Automatic|          3|   null| Yes|\n",
      "|      BMW|      2002| 149572|   Petrol|      Manual|          4| 7018.0| Yes|\n",
      "|   Toyota|      2007| 124180| Electric|   Automatic|          1|   null| Yes|\n",
      "|Chevrolet|      2009|  23888|   Diesel|   Automatic|          2|14694.0| Yes|\n",
      "+---------+----------+-------+---------+------------+-----------+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------------+------------------+---------+------------+------------------+------------------+----+\n",
      "|summary| Brand|       Model Year|           Mileage|Fuel Type|Transmission|       Owner Count|             Price|Sold|\n",
      "+-------+------+-----------------+------------------+---------+------------+------------------+------------------+----+\n",
      "|  count|   146|              160|               160|      145|         142|               160|               144| 160|\n",
      "|   mean|  null|         2010.875|       99960.04375|     null|        null|               2.5|25445.520833333332|null|\n",
      "| stddev|  null|7.181168595008502|55884.347033407095|     null|        null|1.1159224968016033|13945.539691466813|null|\n",
      "|    min|   BMW|             2000|              8373|   Diesel|   Automatic|                 1|            3077.0|  No|\n",
      "|    25%|  null|             2004|             49425|     null|        null|                 2|           12916.0|null|\n",
      "|    50%|  null|             2011|             98610|     null|        null|                 2|           23908.0|null|\n",
      "|    75%|  null|             2017|            143877|     null|        null|                 3|           37560.0|null|\n",
      "|    max|Toyota|             2022|            198824|   Petrol|      Manual|                 4|           49714.0| Yes|\n",
      "+-------+------+-----------------+------------------+---------+------------+------------------+------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3.Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kita pilih fitur/kolom yang bakal kita pake buat prediksi klasifikasinya\n",
    "# select fitur yang kita mau pake + label prediksi (\"Sold\")\n",
    "df_train = df_train.select(\"Mileage\", \"Fuel Type\", \"Transmission\", \"Sold\")\n",
    "df_test = df_test.select(\"Mileage\", \"Fuel Type\", \"Transmission\", \"Sold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Handle Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.na.drop()\n",
    "df_test = df_test.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.withColumn(\"Fuel Type\", when(df_train[\"Fuel Type\"] == \"Diesel\", 1)\\\n",
    "                               .when(df_train[\"Fuel Type\"] == \"Petrol\", 2)\\\n",
    "                               .when(df_train[\"Fuel Type\"] == \"Hybrid\", 3)\\\n",
    "                               .when(df_train[\"Fuel Type\"] == \"Electric\", 4))\n",
    "\n",
    "df_test = df_test.withColumn(\"Transmission\", when(df_test[\"Transmission\"] == \"Automatic\", 1)\\\n",
    "                               .when(df_test[\"Transmission\"] == \"Manual\", 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.withColumn(\"Fuel Type\", when(df_test[\"Fuel Type\"] == \"Diesel\", 1)\\\n",
    "                               .when(df_test[\"Fuel Type\"] == \"Petrol\", 2)\\\n",
    "                               .when(df_test[\"Fuel Type\"] == \"Hybrid\", 3)\\\n",
    "                               .when(df_test[\"Fuel Type\"] == \"Electric\", 4))\n",
    "\n",
    "df_train = df_train.withColumn(\"Transmission\", when(df_train[\"Transmission\"] == \"Automatic\", 1)\\\n",
    "                               .when(df_train[\"Transmission\"] == \"Manual\", 2))\n",
    "\n",
    "df_test = df_test.withColumn(\"Sold\", when(df_test[\"Sold\"] == \"Yes\", 1)\\\n",
    "                               .when(df_test[\"Sold\"] == \"No\", 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------+----+\n",
      "|Mileage|Fuel Type|Transmission|Sold|\n",
      "+-------+---------+------------+----+\n",
      "|  99704|        2|           1| Yes|\n",
      "|  11801|        3|           1| Yes|\n",
      "|  74042|        2|           1| Yes|\n",
      "|  25056|        2|           1|  No|\n",
      "|  12543|        3|           2| Yes|\n",
      "+-------+---------+------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mileage', 'Fuel Type', 'Transmission']\n"
     ]
    }
   ],
   "source": [
    "# NOTE: jangan normalisasi/scale kolom yang kita mau cari (\"Sold\")\n",
    "cols = df_train.columns\n",
    "cols.remove(\"Sold\")\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VectorAssembler(InputCols, outputCol)\n",
    "df_train = VectorAssembler(inputCols = cols, outputCol = \"features\").transform(df_train)\n",
    "df_test = VectorAssembler(inputCols = cols, outputCol = \"features\").transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------+----+-----------------+\n",
      "|Mileage|Fuel Type|Transmission|Sold|         features|\n",
      "+-------+---------+------------+----+-----------------+\n",
      "|  99704|        2|           1| Yes|[99704.0,2.0,1.0]|\n",
      "|  11801|        3|           1| Yes|[11801.0,3.0,1.0]|\n",
      "|  74042|        2|           1| Yes|[74042.0,2.0,1.0]|\n",
      "|  25056|        2|           1|  No|[25056.0,2.0,1.0]|\n",
      "|  12543|        3|           2| Yes|[12543.0,3.0,2.0]|\n",
      "+-------+---------+------------+----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function JavaWrapper.__del__ at 0x7f1938021440>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cloudera/Downloads/]/lib/python3.7/site-packages/pyspark/ml/wrapper.py\", line 40, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "AttributeError: 'BinaryClassificationEvaluator' object has no attribute '_java_obj'\n",
      "Exception ignored in: <function JavaWrapper.__del__ at 0x7f1938021440>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cloudera/Downloads/]/lib/python3.7/site-packages/pyspark/ml/wrapper.py\", line 40, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "AttributeError: 'LogisticRegression' object has no attribute '_java_obj'\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\").fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = scaler.transform(df_train)\n",
    "df_test = scaler.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------------------------------------------------+\n",
      "|features          |scaled_features                                            |\n",
      "+------------------+-----------------------------------------------------------+\n",
      "|[99704.0,2.0,1.0] |[1.7285921935589388,1.8513922928083015,1.9971315421898408] |\n",
      "|[11801.0,3.0,1.0] |[0.20459677120465614,2.777088439212452,1.9971315421898408] |\n",
      "|[74042.0,2.0,1.0] |[1.2836839364066732,1.8513922928083015,1.9971315421898408] |\n",
      "|[25056.0,2.0,1.0] |[0.4344018896113774,1.8513922928083015,1.9971315421898408] |\n",
      "|[12543.0,3.0,2.0] |[0.21746100340818592,2.777088439212452,3.9942630843796816] |\n",
      "|[107634.0,4.0,1.0]|[1.8660765080791424,3.702784585616603,1.9971315421898408]  |\n",
      "|[139415.0,4.0,1.0]|[2.4170713378101123,3.702784585616603,1.9971315421898408]  |\n",
      "|[21896.0,1.0,1.0] |[0.37961621068529366,0.9256961464041508,1.9971315421898408]|\n",
      "|[172280.0,2.0,2.0]|[2.986859735881549,1.8513922928083015,3.9942630843796816]  |\n",
      "|[25358.0,3.0,1.0] |[0.43963773614165497,2.777088439212452,1.9971315421898408] |\n",
      "+------------------+-----------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.select(\"features\", \"scaled_features\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: Column Sold must be of type numeric but was actually of type string.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/Downloads/]/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/]/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1106.fit.\n: java.lang.IllegalArgumentException: requirement failed: Column Sold must be of type numeric but was actually of type string.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.util.SchemaUtils$.checkNumericType(SchemaUtils.scala:76)\n\tat org.apache.spark.ml.PredictorParams$class.validateAndTransformSchema(Predictor.scala:53)\n\tat org.apache.spark.ml.classification.Classifier.org$apache$spark$ml$classification$ClassifierParams$$super$validateAndTransformSchema(Classifier.scala:58)\n\tat org.apache.spark.ml.classification.ClassifierParams$class.validateAndTransformSchema(Classifier.scala:42)\n\tat org.apache.spark.ml.classification.ProbabilisticClassifier.org$apache$spark$ml$classification$ProbabilisticClassifierParams$$super$validateAndTransformSchema(ProbabilisticClassifier.scala:53)\n\tat org.apache.spark.ml.classification.ProbabilisticClassifierParams$class.validateAndTransformSchema(ProbabilisticClassifier.scala:37)\n\tat org.apache.spark.ml.classification.LogisticRegression.org$apache$spark$ml$classification$LogisticRegressionParams$$super$validateAndTransformSchema(LogisticRegression.scala:279)\n\tat org.apache.spark.ml.classification.LogisticRegressionParams$class.validateAndTransformSchema(LogisticRegression.scala:266)\n\tat org.apache.spark.ml.classification.LogisticRegression.validateAndTransformSchema(LogisticRegression.scala:279)\n\tat org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:144)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:100)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-824d836f7ca5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"scaled_features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Sold\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Downloads/]/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/Downloads/]/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/]/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/]/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/]/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed: Column Sold must be of type numeric but was actually of type string.'"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(featuresCol=\"scaled_features\", labelCol=\"Sold\", maxIter = 10).fit(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.select(\"Sold\", \"scaled_features\", \"prediction\").show(15, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol = \"Sold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: {}%\", format(evaluator.evaluate(prediction) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kalo accuracy kurang dari minimal soal:\n",
    "    1. ubah feature yang kalian pilih\n",
    "    2. ubah transform datanya (misal Petrol = 1, Petrol 2)\n",
    "    3. ubah urutan transform datanya(misal Petrol = 1, electric = 2, electric = 1, Petrol = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
